{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POINT BREAKDOWN FOR PSET\n",
    "1. One point for implementing the dataset.\n",
    "2. One point for plotting a batch item queried from the dataset\n",
    "3. One point for implementing the model.\n",
    "4. One point if the model can compute a forward pass without error.\n",
    "5. One point for the training loop executing without error and the loss de-\n",
    "creasing.\n",
    "6. Three points for plotting the modelâ€™s output after training and writing\n",
    "two sentences about why that output is reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import imageio\n",
    "import skimage\n",
    "import h5py\n",
    "import io\n",
    "import requests\n",
    "\n",
    "import torch, torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# # check whether run in Colab\n",
    "# if 'google.colab' in sys.modules:\n",
    "#     print('Running in Colab.')\n",
    "#     !pip3 install timm==0.4.5  # 0.3.2 does not work in Colab\n",
    "#     !git clone https://github.com/facebookresearch/mae.git\n",
    "#     sys.path.append('./mae')\n",
    "# else:\n",
    "#     sys.path.append('..')\n",
    "# import models_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiview functions from Problem set 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogenize_points(points: torch.Tensor):\n",
    "    \"\"\"Appends a \"1\" to the coordinates of a (batch of) points of dimension DIM.\n",
    "\n",
    "    Args:\n",
    "        points: points of shape (..., DIM)\n",
    "\n",
    "    Returns:\n",
    "        points_hom: points with appended \"1\" dimension.\n",
    "    \"\"\"\n",
    "    ones = torch.ones_like(points[..., :1], device=points.device)\n",
    "    return torch.cat((points, ones), dim=-1)\n",
    "\n",
    "\n",
    "def homogenize_vecs(vectors: torch.Tensor):\n",
    "    \"\"\"Appends a \"0\" to the coordinates of a (batch of) vectors of dimension DIM.\n",
    "\n",
    "    Args:\n",
    "        vectors: vectors of shape (..., DIM)\n",
    "\n",
    "    Returns:\n",
    "        vectors_hom: points with appended \"0\" dimension.\n",
    "    \"\"\"\n",
    "    zeros = torch.zeros_like(vectors[..., :1], device=vectors.device)\n",
    "    return torch.cat((vectors, zeros), dim=-1)\n",
    "\n",
    "\n",
    "def unproject(\n",
    "    xy_pix: torch.Tensor,\n",
    "    z: torch.Tensor,\n",
    "    intrinsics: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"Unproject (lift) 2D pixel coordinates x_pix and per-pixel z coordinate\n",
    "    to 3D points in camera coordinates.\n",
    "\n",
    "    Args:\n",
    "        xy_pix: 2D pixel coordinates of shape (..., 2)\n",
    "        z: per-pixel depth, defined as z coordinate of shape (..., 1)\n",
    "        intrinscis: camera intrinscics of shape (..., 3, 3)\n",
    "\n",
    "    Returns:\n",
    "        xyz_cam: points in 3D camera coordinates.\n",
    "    \"\"\"\n",
    "    xy_pix_hom = homogenize_points(xy_pix)\n",
    "    xyz_cam = torch.einsum('...ij,...kj->...ki', intrinsics.inverse(), xy_pix_hom)\n",
    "    xyz_cam *= z\n",
    "    return xyz_cam\n",
    "\n",
    "\n",
    "def transform_world2cam(xyz_world_hom: torch.Tensor, cam2world: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Transforms points from 3D world coordinates to 3D camera coordinates.\n",
    "\n",
    "    Args:\n",
    "        xyz_world_hom: homogenized 3D points of shape (..., 4)\n",
    "        cam2world: camera pose of shape (..., 4, 4)\n",
    "\n",
    "    Returns:\n",
    "        xyz_cam: points in camera coordinates.\n",
    "    \"\"\"\n",
    "    world2cam = torch.inverse(cam2world)\n",
    "    return transform_rigid(xyz_world_hom, world2cam)\n",
    "\n",
    "\n",
    "def transform_cam2world(xyz_cam_hom: torch.Tensor, cam2world: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Transforms points from 3D world coordinates to 3D camera coordinates.\n",
    "\n",
    "    Args:\n",
    "        xyz_cam_hom: homogenized 3D points of shape (..., 4)\n",
    "        cam2world: camera pose of shape (..., 4, 4)\n",
    "\n",
    "    Returns:\n",
    "        xyz_world: points in camera coordinates.\n",
    "    \"\"\"\n",
    "    return transform_rigid(xyz_cam_hom, cam2world)\n",
    "\n",
    "\n",
    "def transform_rigid(xyz_hom: torch.Tensor, T: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply a rigid-body transform to a (batch of) points / vectors.\n",
    "\n",
    "    Args:\n",
    "        xyz_hom: homogenized 3D points of shape (..., 4)\n",
    "        T: rigid-body transform matrix of shape (..., 4, 4)\n",
    "\n",
    "    Returns:\n",
    "        xyz_trans: transformed points.\n",
    "    \"\"\"\n",
    "    return torch.einsum('...ij,...kj->...ki', T, xyz_hom)\n",
    "\n",
    "\n",
    "def get_unnormalized_cam_ray_directions(xy_pix:torch.Tensor,\n",
    "                                        intrinsics:torch.Tensor) -> torch.Tensor:\n",
    "    return unproject(xy_pix, torch.ones_like(xy_pix[..., :1], device=xy_pix.device),  intrinsics=intrinsics)\n",
    "\n",
    "\n",
    "def get_world_rays(xy_pix: torch.Tensor,\n",
    "                   intrinsics: torch.Tensor,\n",
    "                   cam2world: torch.Tensor,\n",
    "                   ) -> torch.Tensor:\n",
    "    # Get camera origin of camera 1\n",
    "    cam_origin_world = cam2world[..., :3, -1]\n",
    "\n",
    "    # Get ray directions in cam coordinates\n",
    "    ray_dirs_cam = get_unnormalized_cam_ray_directions(xy_pix, intrinsics)\n",
    "\n",
    "    # Homogenize ray directions\n",
    "    rd_cam_hom = homogenize_vecs(ray_dirs_cam)\n",
    "\n",
    "    # Transform ray directions to world coordinates\n",
    "    rd_world_hom = transform_cam2world(rd_cam_hom, cam2world)\n",
    "\n",
    "    # Tile the ray origins to have the same shape as the ray directions.\n",
    "    # Currently, ray origins have shape (batch, 3), while ray directions have shape\n",
    "    cam_origin_world = repeat(cam_origin_world, 'b ch -> b num_rays ch', num_rays=ray_dirs_cam.shape[1])\n",
    "\n",
    "    # Return tuple of cam_origins, ray_world_directions\n",
    "    return cam_origin_world, rd_world_hom[..., :3]\n",
    "\n",
    "\n",
    "def get_opencv_pixel_coordinates(\n",
    "    y_resolution: int,\n",
    "    x_resolution: int,\n",
    "    ):\n",
    "    \"\"\"For an image with y_resolution and x_resolution, return a tensor of pixel coordinates\n",
    "    normalized to lie in [0, 1], with the origin (0, 0) in the top left corner,\n",
    "    the x-axis pointing right, the y-axis pointing down, and the bottom right corner\n",
    "    being at (1, 1).\n",
    "\n",
    "    Returns:\n",
    "        xy_pix: a meshgrid of values from [0, 1] of shape\n",
    "                (y_resolution, x_resolution, 2)\n",
    "    \"\"\"\n",
    "    i, j = torch.meshgrid(torch.linspace(0, 1, steps=x_resolution),\n",
    "                          torch.linspace(0, 1, steps=y_resolution))\n",
    "\n",
    "    xy_pix = torch.stack([i.float(), j.float()], dim=-1).permute(1, 0, 2)\n",
    "    return xy_pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the dataset from the paper \"Scene Representation Networks, Sitzmann et al. 2019\" which includes both 3D scenes and camera poses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'cars_train.hdf5' downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import h5py\n",
    "\n",
    "# Local path where you want to save the downloaded file\n",
    "local_file_path = 'cars_train.hdf5'\n",
    "\n",
    "#Download SRNs-cars dataset\n",
    "if not os.path.exists(local_file_path):\n",
    "    # URL of the HDF5 file on the web\n",
    "    url = 'https://drive.google.com/uc?id=1SBjlsizq0sFNkCZxMQh-pNRi0HyFozKb'\n",
    "\n",
    "    # Send an HTTP GET request to download the file\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Open the local file in binary write mode and save the content\n",
    "        with open(local_file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File '{local_file_path}' downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "# import os \n",
    "#Download SRNs-cars dataset in colab \n",
    "#if not os.path.exists(\"/content/cars_train.hdf5\"):\n",
    "    #!gdown 1TIxIBN1EN9FHsH_7_lGFXF9SBYFHn_rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query a single batch from the dataset and plot an element of that batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def parse_rgb(hdf5_dataset):\n",
    "    '''reads the data from the HDF5 dataset'''\n",
    "    s = hdf5_dataset[...].tobytes()\n",
    "    f = io.BytesIO(s)\n",
    "\n",
    "    img = imageio.imread(f)[:, :, :3]\n",
    "    img = skimage.img_as_float32(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def parse_intrinsics(hdf5_dataset):\n",
    "    '''read camera intrinsic parameters'''\n",
    "    s = hdf5_dataset[...].tobytes()\n",
    "    s = s.decode('utf-8')\n",
    "\n",
    "    lines = s.split('\\n')\n",
    "    f, cx, cy, _ = map(float, lines[0].split())\n",
    "    full_intrinsic = torch.tensor([[f, 0., cx],\n",
    "                                    [0., f, cy],\n",
    "                                    [0., 0, 1]])\n",
    "\n",
    "    return full_intrinsic\n",
    "\n",
    "\n",
    "def parse_pose(hdf5_dataset):\n",
    "    '''reads transformation matrices (pose)'''\n",
    "    raw = hdf5_dataset[...]\n",
    "    ba = bytearray(raw)\n",
    "    s = ba.decode('ascii')\n",
    "\n",
    "    lines = s.splitlines()\n",
    "    pose = np.zeros((4, 4), dtype=np.float32)\n",
    "\n",
    "    for i in range(16):\n",
    "        pose[i // 4, i % 4] = lines[0].split(\" \")[i]\n",
    "\n",
    "    pose = torch.from_numpy(pose.squeeze())\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRNsCars(IterableDataset):\n",
    "    def __init__(self, max_num_instances=None, img_sidelength=None, num_images = 4):\n",
    "        self.f = h5py.File('cars_train.hdf5', 'r')\n",
    "        self.instances = sorted(list(self.f.keys()))\n",
    "        print('instances', self.instances)\n",
    "\n",
    "        self.img_sidelength = img_sidelength\n",
    "        self.num_images = num_images\n",
    "\n",
    "        if max_num_instances:\n",
    "            self.instances = self.instances[:max_num_instances]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __iter__(self, override_idx=None): #None\n",
    "        while True:\n",
    "            if override_idx is not None:\n",
    "                idx = override_idx\n",
    "            else:\n",
    "                idx = random.randint(0, len(self.instances)-1)\n",
    "\n",
    "            key = self.instances[idx]\n",
    "\n",
    "            instance = self.f[key]\n",
    "            rgbs_ds = instance['rgb']\n",
    "            c2ws_ds = instance['pose']\n",
    "\n",
    "            rgb_keys = list(rgbs_ds.keys())\n",
    "            c2w_keys = list(c2ws_ds.keys())\n",
    "\n",
    "            obs_idx = np.random.randint(0, len(rgb_keys), self.num_images)\n",
    "\n",
    "            def process_one_img(observation_idx):\n",
    "                rgb = parse_rgb(rgbs_ds[rgb_keys[observation_idx]] )\n",
    "\n",
    "                x_pix = get_opencv_pixel_coordinates(*rgb.shape[:2])\n",
    "\n",
    "                # There is a lot of white-space around the cars - we'll thus crop the images a bit:\n",
    "                rgb = rgb[32:-32, 32:-32]\n",
    "                x_pix = x_pix[32:-32, 32:-32]\n",
    "\n",
    "                # Nearest-neighbor downsampling of *both* the\n",
    "                # RGB image and the pixel coordinates. This is better than down-\n",
    "                # sampling RGB only and then generating corresponding pixel coordinates,\n",
    "                # which generates \"fake rays\", i.e., rays that the camera\n",
    "                # didn't actually capture with wrong colors. Instead, this simply picks a\n",
    "                # subset of the \"true\" camera rays.\n",
    "                if self.img_sidelength is not None and rgb.shape[0] != self.img_sidelength:\n",
    "                    rgb = resize(rgb,\n",
    "                                (self.img_sidelength, self.img_sidelength),\n",
    "                                anti_aliasing=False,\n",
    "                                order=0)\n",
    "                    rgb = torch.from_numpy(rgb)\n",
    "                    x_pix = resize(x_pix,\n",
    "                                (self.img_sidelength, self.img_sidelength),\n",
    "                                anti_aliasing=False,\n",
    "                                order=0)\n",
    "\n",
    "                x_pix = rearrange(x_pix, 'i j c -> (i j) c')\n",
    "                c2w = parse_pose( c2ws_ds[c2w_keys[observation_idx]] )\n",
    "\n",
    "                rgb = rearrange(rgb, 'i j c -> (i j) c')\n",
    "\n",
    "                intrinsics = parse_intrinsics( instance['intrinsics.txt'] )\n",
    "                intrinsics[:2, :3] /= 128. # Normalize intrinsics from resolution-specific intrinsics for 128x128\n",
    "\n",
    "                ###\n",
    "                # Create a dictionary which contains the following\n",
    "                # 1. the 'cam2world' poses, which we computed as c2w\n",
    "                # 2. the camera 'intrinsics'\n",
    "                # 3. the pixel coordinates, 'x_pix'\n",
    "                # 4. the index of the sampled car, 'idx'\n",
    "                model_input = {\n",
    "                        'cam2world': c2w,  # Camera-to-world poses\n",
    "                        'intrinsics': intrinsics,  # Camera intrinsics\n",
    "                        'x_pix': x_pix,  # Pixel coordinates\n",
    "                        'idx': torch.tensor([idx])  # Index of the sampled car\n",
    "                    }\n",
    "                return model_input, rgb \n",
    "            \n",
    "            rgb_list = []\n",
    "            input_list = dict()\n",
    "            for o_idx in obs_idx:\n",
    "                i, r = process_one_img(o_idx)\n",
    "                rgb_list.append(r)\n",
    "                input_list[o_idx] = i\n",
    "\n",
    "            rgb_stacked = torch.stack(rgb_list)\n",
    "            # yield model_input, rgb\n",
    "\n",
    "            yield input_list, rgb_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot single input (N sub images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 32\n",
    "num_images = 5\n",
    "dataset = SRNsCars(img_sidelength=sl,num_images = num_images)\n",
    "mi, rgb = next(iter(dataset))\n",
    "\n",
    "imgs = rgb.view(rgb.shape[0],sl, sl, 3)\n",
    "\n",
    "fig, ax = plt.subplots(1,num_images, figsize=(30, 6))\n",
    "for r in range(num_images):\n",
    "    ax[r].imshow(imgs[r])\n",
    "    ax[r].set_title(f'Input {r}')\n",
    "    # ax[r].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pose_keys = list(mi.keys())\n",
    "print('Scene number:', int(mi[pose_keys[0]]['idx']))\n",
    "print('Pose indices:', pose_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
